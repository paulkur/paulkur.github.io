---
title: Scala-Spark setup
date: 2023-05-24 10:14:00 +0100
categories: [software,docs,scala,setup,readme]
tags: [software,docs,scala,spark,setup,readme]     # TAG names should always be lowercase
pin: false
---

Hi ! üëã and Welcome

## Menu

- [Menu](#menu)
- [How to setup Scala-Spark local environment](#how-to-setup-scala-spark-local-environment)
  - [Windows users üë©‚Äçüíª](#windows-users-)
  - [MacOS \& Linux users üë©‚Äçüíª](#macos--linux-users-)
  - [Possible issues and fixes](#possible-issues-and-fixes)
  - [create "hello-world" demo in Scala 3](#create-hello-world-demo-in-scala-3)
  - [Learning resources](#learning-resources)

## How to setup Scala-Spark local environment

### Windows users üë©‚Äçüíª

- Install Docker üê≥ for [Windows](https://docs.docker.com/desktop/install/windows-install/)

> Windows users: Docker needs Windows Hyper-V to work.
{: .prompt-tip }

- In a terminal window, navigate to the location of the `assignment.zip`

> Replace `<USERNAME>` with your PC username
{: .prompt-tip }

```powershell
cd C:\Users\<USERNAME>\Downloads
```

and run:

```powershell
Expand-Archive -Path "assignment.zip"
Remove-Item -Path "assignment.zip"
cd \assignment\spark-cluster
.\build-images.bat
```

- Spin up the Spark üí• containers with 3 worker nodes `spark-workers` (can change `3` to more or less, depending on hardware)

```powershell
docker-compose up -d --scale spark-worker=3
```

> A Note For Windows users: Adding Winutils
> By default, Spark will be unable to write files using the local Spark executor. To write files, you will need to install the Windows Hadoop binaries, aka [winutils](https://github.com/cdarlint/winutils). Used Hadoop 2.7 as a fallback.
> After you download winutils.exe, create a directory anywhere (e.g. `C:\\winutils`), then create a `bin` directory under that, then place the winutils executable there.
> You will also need to set the `HADOOP_HOME` environment variable to your directory where you added `bin\winutils.exe`. In the example above, that would be `C:\\winutils`.
> An alternative to setting the environment variable is to add this line at the beginning of every Spark application we write:

```scala
System.setProperty("hadoop.home.dir","C:\\hadoop") // replace C:\\hadoop with your actual directory
```

{: .prompt-tip }

- Open with IntelliJ as an SBT project

Folder üìÇ structure is as follows:

```tree
assignment
  src
   main
     scala
       prod
        model.scala
        q1.scala
        q2.scala
        q3.scala
        q4.scala
```

- CSV output files can be found in:

```tree
assignment
  src
   main
     resources
       data
         exports
          names-in-projects
          names-in-projects
          names-in-projects
```

- To run them, just open up intelij, navigate to Scala main, and runüèÉ‚Äç‚ôÄÔ∏è

![Start the code](/assets/img/start.png){: width="972" height="589" }
_Setup for a start_

- Clean/Remove the Spark-Cluster

```bash
cd .. && .\docker-clean.sh
```

### MacOS & Linux users üë©‚Äçüíª

- Install Docker üê≥ for [MacOS](https://docs.docker.com/desktop/install/mac-install/) or [Linux](https://docs.docker.com/desktop/install/linux-install/)

- in a terminal window, navigate to the location of the `assignment.zip`

> Replace `<USERNAME>` with your PC username
{: .prompt-tip }

```bash
cd /Users/<USERNAME>/Downloads
```

and run:

> Make sure Docker application is running : Strat from UI
{: .prompt-tip }

```bash
unzip assignment.zip -d ./
rm assignment.zip
cd \assignment\spark-cluster
chmod +x build-images.sh && ./build-images.sh
```

- Spin up the Spark üí• containers with 3 worker nodes `spark-workers` (can change `3` to more or less, depending on hardware)

```bash
docker-compose up -d --scale spark-worker=3
```

- Open with IntelliJ as an SBT project

Folder üìÇ structure is as follows:

```tree
assignment
  src
   main
     scala
       prod
        model.scala
        q1.scala
        q2.scala
        q3.scala
        q4.scala
```

- CSV output files can be found in:

```tree
assignment
  src
   main
     resources
       data
         exports
          names-in-projects
          names-in-projects
          names-in-projects
```

- To run them, just open up intelij, navigate to Scala main, and run üèÉ‚Äç‚ôÄÔ∏è

![Start the code](/assets/img/start.png){: width="972" height="589" }
_Setup for a start_

- Clean/Remove the Spark-Cluster

```bash
cd .. && .\docker-clean.sh
```

[Back to Top](#menu)

### Possible issues and fixes

Make sure the ownership of `~/.docker` is correct

```bash
ls -ld ~/.docker
```

```bash
sudo chown -R $USER ~/.docker
```

Add docker to your USERGROUP

MacOS

```bash
sudo dseditgroup -o edit -a $USER -t user docker
```

Linux

```bash
sudo usermod -aG docker $USER
```

### create "hello-world" demo in Scala 3

```bash
sbt new scala/hello-world.g8
```

```bash
cd <project_name>
sbt test
```

on scala 3

```bash
sbt new scala/scala3.g8
```

### Learning resources

from main sugestions:

- [Martin Odersky, "Working Hard to Keep It Simple"](https://www.youtube.com/watch?v=3jg1AheF4n0&t=371s)
- [Programming in Scala](http://www.artima.com/pins1ed/)
- [Scala Cookbook](http://scalacookbook.com/)
- [Programming in Scala](http://www.artima.com/pins1ed/)
- [Oreilly - Learning Scala](https://www.oreilly.com/library/view/learning-scala/9781449368814/)
- Coursera course on [Functional programming in Scala](https://www.coursera.org/learn/progfun1)

from new found sugestions:

{% include embed/youtube.html id='3jg1AheF4n0&t=371s' %}

[Back to Top](#menu)
